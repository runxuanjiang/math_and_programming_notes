# Embeddings & Vector Stores

Notes based on the [whitepaper here](https://www.kaggle.com/whitepaper-embeddings-and-vector-stores).


## Embeddings

**Embeddings** refer to numerical representations (usually vectors) of real-world data (text, speech, image, videos).

* Usually embedded as low-dimensional vectors (so the mapping is lossy).
* Distance between two vectors represent relationship adn semantic similarity between the real-world objects.
    * Example: "computer" might have a similar embedding to a picture of a computer or the word "laptop".
* Usually generated by neural network models.

Applications of embeddings include:
* Retrieval and recommendations - finding results based on a specific search.
    * Recommender systems
    * Search engines
* Multimodal data - **joing embeddings** refers to when different modalities of data (text, speech, etc.) map to the same embedding space.
* Semantic text similarity - paraphrasing, duplicate detection
* Classification

For an end-to-end example see the section "Search Example" in the whitepaper.

###  Evaluating embeddings

Evaluation often focus on the ability to retrieve similar items, and require a dataset with labelled datapoints. 

**Precision** measures whether all data retrieved are relevant, and is calculated using `relevant data items retrieved / total items retrieved`.

**Recall** measures whether all relevant data items are retrieved, calculated using `relevant data items retrieved / total number of relevant items`.

Precision and recall only measure the binary success of the model. To measure whether the model is able to correctly determine what data is more relevant than others, metrics like the **Normalized Discounted Cumulative Gain (nDCG)** is used, which is calculated by

$$DCG = \sum_{i=1}^p \frac{rel_i}{\log_2 (i+1)}$$

Here, $p$ is the number of data items retrieved and the documents are sorted in order of decreasing priority. $rel_i$ is a relevancy score, and the denominator is used to penalize documents for being lower on the list.

Public benchmarks include:
* BEIR
* Massive Text Embedding Benchmark (MTEB)

Standard libraries for benchmarking:
* trec_eval
* pytrec_eval

### Types of Embeddings

####  Text embeddings


Basic text embeddings can be generated as follows:
* **Tokenization** - the input text is split into smaller pieces called tokens.
    * Can be wordpieces, characters, words, numbers, punctuations.
    * Tokenization depends on the specific tokenization technique used.
* Indexing: each unique token is assigned an "index", between 0 and the number of unique tokens.
* Embedding: the input is represented as a one-hot encoded binary vector, where the set bits correspond to the indices representing the tokens in the input.



Word embeddings:
* Word2Vec
    * Uses the principle that the semantic meaning of a word is defined by its neighbors in the training data.
    * Each word is embedded to a fix length vector.
    * For training, each embedding starts as a random vector. There are two approaches to training:
        * **Continuous bag of words (CBOW)**: Try to predict the middle word in a sentence, using the embeddings of surrounding words as input.
        * **skip-gram**: try to predict surrounding words within a certain range using the middle word's embedding as input.
        * skip-gram takes longer to train but is more accurate for rare words.
    * Does not capture global statistics (how each word fits within the entire corpus), only local statistics within some sliding window.
* GloVe
    * Leverages both local and global statistics of words.
    * Creates a co-occurrences matrix for words, then learns word representations from the matrix.
* SWIVEL
    * Similar to GloVe where it uses a co-occurences matrix, but uses local windows to learn vectors.
    * Faster to train than GloVe, but slightly less accurate.



Document embeddings:

Originally done using shallow Bag-of-words (BOW) models.
* Documents are treated as unordered collection of words.
* Example techniques:
    * Latent semantic analysis (LSA) - uses co-occurence matrix of words in document to generate embeddings.
    * Latent dirichlet allocation (LDA) - uses bayesian network to model document embeddings
    * TF-IDF - use word frequencies to generate embeddings
* The bag-of-words model's weakness is that semantic meaning and word ordering is ignored.

Doc2Vec tries to address the word ordering issue - uses a similar technique as Word2Vec, but for documents. Adds an additional "paragraph" embedding.

More recent techniques use deeper pretrained large language models.
* BERT was released in 2018 which uses transformers.
    * Bi-directional transformer allows it to use both left and right context.
    * Was pre-trained on a massive unlabeled corpus.
        * This was done by using masked language model (MLM) as the pre-training objective - tokens were randomly masked in the input and the objective was to predict the masked tokens.
    * Used as base model for many other models:
        * Sentence-BERT
        * SimCSE
        * E5
* More recent models use more parameters include
    * PaLM
    * Gemini
    * GPT
    * Llama
* Some models embeds words into multiple vectors, to enhance the representational power of the embeddings. These include:
    * colBERT
    * XTR

#### Image and multimodal embeddings

Unimodal image embeddings can be derived by training a CNN on an image classification task and then using the final layer as the embedding.

Multimodal embeddings (embeddings across multiple modalities) are derivated by taking unimodal text and image embeddings and then train another model on a task that combines both modalities.


#### Structured data embeddings

Structured data refers to a defined schema, like a database table.

Embeddings can be created using dimensionality reduction techniques, like PCA.


#### Graph embeddings
For graph data, common techniques include using each node as an embedding, while capturing the relationships in the edges. Example algorithms include
* DeepWalk
* Node2vec
* LINE
* GraphSAEG


### Training Embeddings

Modern embedding models usually use two encoders. For example
* One encoder for queries, one encoder for documents
* One encoder for images, one encoder for text

Contrastive loss function is used, which takes in inputs, positive targets, and negative targets, to derive a loss that encourages positive examples to be closer and negative examples to be far apart (in terms of embedding distance).

Training embedding models involve a pretraining stage and a fine tuning stage:
* pretraining - uses unsupervised learning to derive embeddings. This is usually done already when using a model as the base, like BERT.
* fine-tuning - uses supervised learning to train extra layers for specific tasks, like classification.


## Vector Search

Vector search involves using vector embeddings to search for text or other data. Each dataset is stored as an embedding in the database. When a user makes a query, it is turned into an embedding. The data in the database that is closest to the query embedding are returned.

"Closest" can be measured in a few different ways
* L2 distance
* Cosine similarity
* Inner product

### Vector search algorithms

The naive solution to vector search is to do a linear search across all the data and sort the results based on one of the similarity metrics. However this is very slow.

**Aprproximate nearest neighbor (ANN)** can find the closet points with a small margin of error, with O(logN) complexity.

**Locality sensitive hashing (LSH)** creates one or more hash functions that maps similar items to the same hash with high probability - so similar items can be found by looking at candidate items in the same hash bucket.

Tree-based algorithms like **Kd-tree** work similarly by creating decision boundaries to separate the data into groups, based on the values of the embeddings in the first few dimensions (such as using median to divide the group into two sets).

**Hierarchical navigable small worlds (HNSW)** uses a multi-layer proximity graph to find the k-nearest neighbors in logarithmic time.

**ScaNN** uses partitioning and pruning to perform ANN efficiently.


### Vector databases
Vector databases manage querying embeddings in a production scenario. The general flow includes:
* A trained embedding model is used to embed new data points.
* The embeddings are augmented with metadata and indexed (using one of the search algorithms above, for example) for efficient search.
* Incoming queries are embedding using the embedding model, and a search algorithm is used to search for semantically similar items.

Examples include:
* Google Cloud Vertex Vector Search
* AlloyDB
* Pinecone
* Weaviate
* ChromaDB

There are additional operational considerations associated with vector databases (in addition to standard ones like availability and data consistency):
* Embeddings can mutate over time - embedding model will improve and embeddings need to be regenerated. It's useful to have multiple emebddings for the same data, using different models.
* Vector databases are suboptimal for literal or syntactic information, like domain-specific words or ID's.
* 


## Further reading
* [Gecko paper](https://arxiv.org/abs/2403.20327) - explains how to use LLM's to generate labeled data for training embedding model.